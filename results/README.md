# Results reported in the Text2VQL paper

This section is dedicated to reproduce all the figures, tables, statistics, etc. reported in the paper
to support our answers of the RQs.

```bash
cd results
```

## Dependencies

The dependencies of this phase are basically Python data science libraries.
```bash
conda create --name text2vql-results python=3.8
conda activate text2vql-results
pip install -r requirements.txt
```

## Results structure

The results structure is composed by several important directories:
* `ai` contains the csv with the generated queries by each LLM.
* `annotation` contains the csv with the author annotations about the quality of the generated queries (see RQ1). The columns
`Reviewer1` and `Reviewer2` indicate the labels given by the first and the second reviewer. The column `Agreed` is the final decision
according to the methodology presented in the paper.
* `eval` contains the results of running the test framework over the queries generated by the LLMs. Particularly, the column
`has_correct` indicates if one of the generated queries for a given test specification is correct (syntactically valid and passes all the tests).
* `merged` contains the csv that are generated from merging the csvs of `eval` and `ai`.
* `profiles` contains the counts about the constructs of the generated queries and the test queries (used to answer RQ2).
* `testmodels` contains the instance models used to test the validity of the generated queries. Note that one query is considered
correct if 1) it is syntactically valid, and 2) for all the instance models the returned elements are the same as the ground truth query.

**IMPORTANT NOTE ðŸ¤”**
All folders (`ai`, `annotation`, `eval`, etc.) contains the results that we have obtained when performing our experiments and they are the ones
reported in the paper. Therefore, if you have run scripts of the previous phases, you may overwrite some of the csvs. Furthermore,
if you directly run the RQs scripts (i.e., `present_results_rqi.py` or `present_results_rqi_j.py`), you will obtain the results
reported in the paper.

## RQ1: Quality of the generated queries

By running the following script we compute 1) the kappa agreement score between the reviewers and 2) the distribution of labels among the
labelled queries.
```bash
python present_results_rq1.py
```

## RQ2: Complexity and VQL coverage 

To check which language elements were encountered in the dataset run the following commands.
The first command compiles the Xtend source files. You may skip it, **if** it was compiled in a previous step, and no changes were made to the code.
The second command executes the profiler in aggregate mode and prints the results to the standard output.
Interpret the output as a tree, e.g., `#contraints=39556 \n\t #comapre=3482` is the number of compare constraints.

```bash
docker exec -it -u abc \
    -w /config/eclipse-workspace/se.liu.ida.sas.pelab.vqlsyntaxcheck \
    eclipse-vnc java -cp "/opt/eclipse/plugins/*:jdbc/*" org.eclipse.xtend.core.compiler.batch.Main \
    -d xtend-gen -useCurrentClassLoader src
docker exec -it -u abc \
    -e MODE=AGG \
    -e CSV=/config/text2vql/results/profiles/final_dataset.csv \
    -e COL=pattern \
    -w /config/eclipse-workspace/se.liu.ida.sas.pelab.vqlsyntaxcheck \
    eclipse-vnc ant clean build ProfileMain
```

The next commands create the csv files containing the non-aggregated properties of the queries, used in the query complexity evaluation.
You may skip the first `docker exec` if no changes were made to the code.
```bash
docker exec -it -u abc \
    -w /config/eclipse-workspace/se.liu.ida.sas.pelab.vqlsyntaxcheck \
    eclipse-vnc java -cp "/opt/eclipse/plugins/*:jdbc/*" org.eclipse.xtend.core.compiler.batch.Main \
    -d xtend-gen -useCurrentClassLoader src
docker exec -it -u abc \
    -e MODE=IND \
    -e CSV=/config/text2vql/results/profiles/final_dataset.csv \
    -e COL=pattern \
    -e OUT=/config/text2vql/results/profiles/profiles_raw.csv \
    -w /config/eclipse-workspace/se.liu.ida.sas.pelab.vqlsyntaxcheck \
    eclipse-vnc ant clean build ProfileMain

python merge-profile.py
```
As refinery is a fairly large piece of software, we direct your attention to the `generator-cli` subproject, as that contains the domain specific parts. The resources directory (it contains the problem specifications, that the synthesis task will use), the `Text2VQLTestGenerator` class (responsible for running the test genration), and the Problem2RAilway java file, that is responsible for attributes and mapping the solution to EMF.

Once the profiles/counts are generated, the following scripts plots the distribution of effective lines (reported in the paper)
and counts by number of constructs (also reported in the paper).
```bash
python present_results_rq2.py
```

## RQ3: Fine-tuning with the synthetic dataset

(Optional.) Generate instance models in the Railway domain for testing.
This will generate 300 models will a seed model, and an additional 300 random model. 
Warning: This may replace the original models we used. Expected runtime is around 3-6 hours.

```bash
docker exec -it -u abc \
    -w /config/refinery \
    eclipse-vnc ./generate_models.sh /config/text2vql/results/testmodels 
```

To execute the query comparison test, use the following docker commands. 
This will run the match set comparison for all csvs in the `ai` directory.
The expected time of the evaluation is 30-60 minutes.
Results will be saved to the `eval` and `merged` directories.
```bash
docker exec -it -u abc \
    -e INSTANCEDIR=/config/text2vql/results/testmodels \
    -e META=/config/text2vql/dataset_construction/test_metamodel/railway.ecore \
    -e INDIR=/config/text2vql/results/ai/ \
    -e OUTDIR=/config/text2vql/results/eval/ \
    -w /config/util \
    eclipse-vnc ./compare.sh

python merge-eval.py
```

Once the generated queries have been evaluated, we can compute the Pass@5 score reported in the paper for each LLM.
```bash
python present_results_rq3_1.py --csv_results merged/deepseek-ai-deepseek-coder-6.7b-base_fs_random_full.csv
python present_results_rq3_1.py --csv_results merged/deepseek-ai-deepseek-coder-6.7b-base_lora_full.csv 

python present_results_rq3_1.py --csv_results merged/deepseek-ai-deepseek-coder-1.3b-base_fs_random_full.csv
python present_results_rq3_1.py --csv_results merged/deepseek-ai-deepseek-coder-1.3b-base_lora_full.csv

python present_results_rq3_1.py --csv_results merged/codellama-CodeLlama-7b-hf_fs_random_full.csv
python present_results_rq3_1.py --csv_results merged/codellama-CodeLlama-7b-hf_lora_full.csv 

python present_results_rq3_1.py --csv_results merged/chatgpt_fs_random_full.csv
```

Finally, to support our claim of the fact that the trained LLMs struggle with complex queries, the first command generates
the complexity profile of the ground truth queries, and the following split produces
a boxplot that compares the effective lines of the group of correct and incorrect queries 
(considering, by default, `deepseek-ai-deepseek-coder-6.7b-base`).

```bash
docker exec -it -u abc \
    -w /config/eclipse-workspace/se.liu.ida.sas.pelab.vqlsyntaxcheck \
    eclipse-vnc java -cp "/opt/eclipse/plugins/*:jdbc/*" org.eclipse.xtend.core.compiler.batch.Main \
    -d xtend-gen -useCurrentClassLoader src
docker exec -it -u abc \
    -e MODE=IND \
    -e CSV=/config/text2vql/dataset_construction/test_metamodel/test_queries.csv \
    -e COL=truth \
    -e OUT=/config/text2vql/results/profiles/profiles_truth_raw.csv \
    -w /config/eclipse-workspace/se.liu.ida.sas.pelab.vqlsyntaxcheck \
    eclipse-vnc ant clean build ProfileMain

python merge-truth.py
```

```bash
python present_results_rq3_2.py
```
